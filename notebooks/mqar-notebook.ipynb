{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/cr05_data/sim_data/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from associative_recall import _gap_power_distr_ar\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from torch.fft import rfft, irfft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting MQAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 25,  91,  40,  87,  16,  89,  32,  95,  44, 121,  55,  73,  19,  86,\n",
      "         41, 123,  50,  16,  43,  25,  12,  40,   8,  78,  66,   6, 105,  44,\n",
      "         60,  41,  48,  55,  69,  78,  58, 101,  24, 112,  64, 127, 114,   5,\n",
      "         73,   4,  17, 101,  63,  46,   7,  49,  48,   9, 106,  19,  58,  25,\n",
      "         84,  32,  52, 104,   0, 104,  61,  64])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100,   89, -100,   91, -100,   87, -100, -100,\n",
      "        -100, -100, -100,  121, -100,  123, -100,   73, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100,   86, -100, -100, -100,   95, -100, -100,\n",
      "        -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 128\n",
    "input_seq_len=64\n",
    "num_examples=5\n",
    "num_kv_pairs=8\n",
    "model_dimension = input_seq_len * vocab_size\n",
    "context_size = num_kv_pairs * 2\n",
    "\n",
    "data = _gap_power_distr_ar(\n",
    "    vocab_size=vocab_size,\n",
    "    input_seq_len=input_seq_len,\n",
    "    num_examples=num_examples,\n",
    "    num_kv_pairs=num_kv_pairs,\n",
    "    seed=123\n",
    ")\n",
    "inputs = data[0] \n",
    "targets = data[1]\n",
    "\n",
    "print(inputs[0])\n",
    "print(targets[0])\n",
    "\n",
    "# Understanding this data -- -100's are not scored. Notice in the first portion of the sequence, there are kv pairs (25, 91) (40, 87) etc. and then later we see in the targets a \"91\" and a \"87\". At the corresponding positions in the input, we see \"25\" followed by a random token \"22\" and \"99\" followed by a random token \"40\". The model needs to predict the non- -100 tokens in the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample solution to MQAR with autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring Function\n",
    "def run_check(y, label, embeddings, input):\n",
    "    preds = (embeddings.weight.data @ y[0]).argmax(dim=0)\n",
    "    print(f\"{input=}\")\n",
    "    print(f\"{preds=}\")\n",
    "    print(f\"{label=}\")\n",
    "    result = (label == preds)[label != -100].to(float).mean()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_convolution(x: torch.Tensor, k: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (d, l)\n",
    "        k: (d, l)\n",
    "    \"\"\"\n",
    "    seqlen = x.shape[-1]\n",
    "    fft_size = seqlen * 2\n",
    "    x_f = torch.fft.rfft(x, n=fft_size)\n",
    "    k_f = torch.fft.rfft(k, n=fft_size)\n",
    "    y_f = x_f * k_f\n",
    "    y = torch.fft.irfft(y_f, n=fft_size)[..., :seqlen]\n",
    "    return y\n",
    "\n",
    "def fft_autocorrelation(q: torch.Tensor, k: torch.Tensor=None):\n",
    "    if k is None:\n",
    "        k = q\n",
    "    fft_len = q.shape[-1] * 2\n",
    "    return irfft(rfft(q, n=fft_len)  * torch.conj(rfft(k, n=fft_len)), n=fft_len)[..., :q.shape[-1]]\n",
    "\n",
    "def get_top_k(q, top_k=3):\n",
    "    z = fft_autocorrelation(q, q)\n",
    "    z[..., 0] = 0  # this is back to when the sequence is the same (What does this line do?)\n",
    "    z = torch.sum(z, dim=1)\n",
    "    _, indices = torch.topk(z, k=top_k, sorted=False, dim=-1)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input=tensor([ 7,  3,  6,  1,  6,  1,  7,  3,  4,  0,  6, 14])\n",
      "preds=tensor([3, 6, 7, 3, 6, 1, 3, 6, 1, 3, 1, 0])\n",
      "label=tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    1, -100])\n",
      "result=tensor(1., dtype=torch.float64)\n",
      "\n",
      "input=tensor([4, 0, 6, 1, 4, 0, 7, 3, 6, 2])\n",
      "preds=tensor([6, 6, 2, 4, 0, 6, 0, 6, 1, 4])\n",
      "label=tensor([-100, -100, -100, -100, -100, -100, -100, -100,    1, -100])\n",
      "result=tensor(1., dtype=torch.float64)\n",
      "\n",
      "input=tensor([6, 1, 7, 2, 4, 0, 9, 9, 9, 7, 2, 6, 1, 4, 7])\n",
      "preds=tensor([7, 2, 7, 1, 6, 2, 5, 3, 6, 1, 7, 1, 7, 0, 9])\n",
      "label=tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100,    0, -100])\n",
      "result=tensor(1., dtype=torch.float64)\n",
      "\n",
      "input=tensor([4, 2, 6, 1, 7, 3, 6, 8])\n",
      "preds=tensor([4, 2, 6, 1, 7, 3, 1, 7])\n",
      "label=tensor([-100, -100, -100, -100, -100, -100,    1, -100])\n",
      "result=tensor(1., dtype=torch.float64)\n",
      "\n",
      "score/len(examples)=tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# example inputs\n",
    "examples = [\n",
    "    [torch.tensor([7, 3, 6, 1, 6, 1, 7, 3, 4, 0, 6, 14]), 1],   # 1 is th next-token prediction for \"6\"\n",
    "    [torch.tensor([4, 0, 6, 1, 4, 0, 7, 3, 6, 2]), 1],\n",
    "    [torch.tensor([6, 1, 7, 2, 4, 0, 9, 9, 9, 7, 2, 6, 1, 4, 7,]), 0],\n",
    "    [torch.tensor([4, 2, 6, 1, 7, 3, 6, 8]), 1],\n",
    "]\n",
    "\n",
    "score = 0\n",
    "for ex in examples:\n",
    "    input, label = ex\n",
    "    label = torch.tensor([-100]*(len(input)-2) + [label, -100])\n",
    "    \n",
    "    assert len(input) == len(label)\n",
    "    length, batch, dim, top_k = len(input), 1, 16, 2\n",
    "    vocab_size = max(input.tolist() + label.tolist()) + 1\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # embed the input\n",
    "    embeddings = nn.Embedding(vocab_size, dim)\n",
    "    x = embeddings(input).T\n",
    "    batch_x = x.unsqueeze(0).repeat(batch, 1, 1)\n",
    "    q, v, z = batch_x, batch_x, batch_x\n",
    "\n",
    "    # compute autocorrelation and get the top-k gaps that we want to shift for\n",
    "    indices = get_top_k(q, top_k=top_k)\n",
    "\n",
    "    # construct filters for each of the top-k gaps to pull keys and values forward\n",
    "    filters_keys = []\n",
    "    filters_values = []\n",
    "    for j in range(indices.shape[-1]):\n",
    "        filter_value = torch.zeros(batch, length).to(device=q.device, dtype=q.dtype)\n",
    "        filter_key = torch.zeros(batch, length).to(device=q.device, dtype=q.dtype)\n",
    "        filter_key[torch.arange(batch), indices[:, j]] = 1            # conv to pull forward keys\n",
    "        filter_value[torch.arange(batch), indices[:, j]-1] = 1       # conv to pull forward values\n",
    "        filters_keys.append(filter_key)\n",
    "        filters_values.append(filter_value)\n",
    "    key_filt = torch.stack(filters_keys, dim=1)\n",
    "    value_filt = torch.stack(filters_values, dim=1)\n",
    "\n",
    "    # apply the filters\n",
    "    keys = fft_convolution(                                                     \n",
    "        rearrange(q, \"b d l -> b d 1 l\"), \n",
    "        rearrange(key_filt, \"b h l -> b 1 h l\")\n",
    "    )\n",
    "    values = fft_convolution(                                                     \n",
    "        rearrange(q, \"b d l -> b d 1 l\"), \n",
    "        rearrange(value_filt, \"b h l -> b 1 h l\")\n",
    "    )\n",
    "\n",
    "    # apply a mask to select which gaps map to a particular query token\n",
    "    mask = torch.softmax(torch.einsum(\"b d l, b d h l -> b h l\", q, keys), dim=1)\n",
    "    y = torch.einsum(\"b d h l, b h l -> b d l\", values, mask)     \n",
    "\n",
    "    # applies an \"argmax\" lm head and determines whether the \"mlm\" prediction for the target token is correct                            \n",
    "    result = run_check(y, label, embeddings, input)\n",
    "    print(f\"{result=}\\n\")\n",
    "    score += result\n",
    "\n",
    "# overall averaged score\n",
    "print(f\"{score/len(examples)=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
